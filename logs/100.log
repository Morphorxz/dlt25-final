
==================================================
Initializing with parameters: (np.float64(2.776247174936222), np.float64(0.6069341493735683), np.float64(0.4421040465339125), np.float64(456.8695537103817), 1.0, 0.5, 0.5)
==================================================

Step    0: Loss=0.018381, Best Loss=0.018381, Grad Norm=1.72e-01
Parameters: L0=2.7249, A=0.5566, alpha=0.4471, B=456.5911, C=0.9495, beta=0.4950, gamma=0.4950

Step   10: Loss=0.002123, Best Loss=0.002123, Grad Norm=8.28e-02
Parameters: L0=2.6761, A=0.5407, alpha=0.4587, B=454.2884, C=1.0772, beta=0.4986, gamma=0.5071

Step   20: Loss=0.002362, Best Loss=0.001127, Grad Norm=1.75e-01
Parameters: L0=2.6574, A=0.5767, alpha=0.4709, B=452.0522, C=1.3198, beta=0.5114, gamma=0.5331

Step   30: Loss=0.001383, Best Loss=0.000578, Grad Norm=1.63e-01
Parameters: L0=2.6601, A=0.6154, alpha=0.4761, B=449.8577, C=1.5092, beta=0.5241, gamma=0.5551

Step   40: Loss=0.001091, Best Loss=0.000578, Grad Norm=1.44e-01
Parameters: L0=2.6526, A=0.5982, alpha=0.4713, B=447.6682, C=1.5920, beta=0.5309, gamma=0.5656

Step   50: Loss=0.000503, Best Loss=0.000365, Grad Norm=8.65e-02
Parameters: L0=2.6623, A=0.5938, alpha=0.4634, B=445.5457, C=1.7029, beta=0.5429, gamma=0.5823

Step   60: Loss=0.000321, Best Loss=0.000311, Grad Norm=3.36e-02
Parameters: L0=2.6544, A=0.5968, alpha=0.4628, B=443.4546, C=1.8598, beta=0.5599, gamma=0.6067

Step   70: Loss=0.000287, Best Loss=0.000287, Grad Norm=2.28e-02
Parameters: L0=2.6553, A=0.5960, alpha=0.4579, B=441.3993, C=1.9806, beta=0.5753, gamma=0.6270

Step   80: Loss=0.000277, Best Loss=0.000277, Grad Norm=8.83e-03
Parameters: L0=2.6532, A=0.6015, alpha=0.4555, B=439.3624, C=2.0798, beta=0.5894, gamma=0.6450

Step   90: Loss=0.000283, Best Loss=0.000275, Grad Norm=2.75e-02
Parameters: L0=2.6511, A=0.6024, alpha=0.4521, B=437.3427, C=2.1530, beta=0.6013, gamma=0.6594

Step  100: Loss=0.000282, Best Loss=0.000275, Grad Norm=1.16e-02
Parameters: L0=2.6487, A=0.6051, alpha=0.4496, B=435.3404, C=2.2119, beta=0.6118, gamma=0.6719

Early stopping at step 107: No improvement for 20 steps.
Train Set Evaluation:
cosine_24000.csv
huber_loss: 0.00011608230725651654
mse_loss: 1.949714161156523e-05
rmse_loss: 0.004415556772544685
mae_loss: 0.0032781051570879276
prede: 0.001067425915258707
worste: 0.0037060555645734637
r2_score: 0.9993365158371322
constant_24000.csv
huber_loss: 3.639274344041545e-05
mse_loss: 4.27678835365961e-06
rmse_loss: 0.0020680397369633907
mae_loss: 0.0018486950257198622
prede: 0.0005860237425213507
worste: 0.00138958734757789
r2_score: 0.9998136811558801
wsdcon_9.csv
huber_loss: 0.0001284872251259708
mse_loss: 4.021885241776146e-05
rmse_loss: 0.00634183352176336
mae_loss: 0.004934622952946522
prede: 0.0015791912651248173
worste: 0.005465802309113901
r2_score: 0.9986414862157197
Average Huber_loss: 9.365409194096758e-05
Average Mse_loss: 2.1330927460995432e-05
Average Rmse_loss: 0.0042751433437571455
Average Mae_loss: 0.0033538077119181033
Average Prede: 0.001077546974301625
Average Worste: 0.003520481740421752
Average R2_score: 0.9992638944029107
--------------------------------------------------
Test Set Evaluation:
constant_72000.csv
huber_loss: 0.0006247738389980604
mse_loss: 3.209226080802903e-05
rmse_loss: 0.00566500316046064
mae_loss: 0.004776167134463194
prede: 0.0015839410972218763
worste: 0.006637749359482806
r2_score: 0.9979590918716806
cosine_72000.csv
huber_loss: 0.0010756190743544088
mse_loss: 6.738459098459398e-05
rmse_loss: 0.00820881178883972
mae_loss: 0.007265125913143878
prede: 0.0024507701965283644
worste: 0.00454538605539939
r2_score: 0.996986831109932
wsd_20000_24000.csv
huber_loss: 0.00014439362466463557
mse_loss: 3.4991190756380795e-05
rmse_loss: 0.005915335219273781
mae_loss: 0.0037480663459463666
prede: 0.001226203097266153
worste: 0.005231052091873211
r2_score: 0.9986681580580885
wsdld_20000_24000.csv
huber_loss: 0.0001152542530168217
mse_loss: 2.4240558380713474e-05
rmse_loss: 0.004923470156374818
mae_loss: 0.003248309560567043
prede: 0.001056406952616881
worste: 0.005716078187630216
r2_score: 0.9990444099542846
wsdcon_3.csv
huber_loss: 0.00015446826880152547
mse_loss: 7.913619978329618e-05
rmse_loss: 0.008895852954230763
mae_loss: 0.005669397683181635
prede: 0.0018060716351643196
worste: 0.010335466563811464
r2_score: 0.9972808029036484
wsdcon_18.csv
huber_loss: 1.6716994667962823e-05
mse_loss: 3.6283231503833633e-06
rmse_loss: 0.0019048157785947078
mae_loss: 0.0013816388567887683
prede: 0.0004256385865180646
worste: 0.0025100590379261182
r2_score: 0.9998656433628628
Average Huber_loss: 0.0003552043424172358
Average Mse_loss: 4.024552064389947e-05
Average Rmse_loss: 0.0059188815096290716
Average Mae_loss: 0.004348117582348481
Average Prede: 0.0014248385942192765
Average Worste: 0.005829298549353867
Average R2_score: 0.9983008228767494
--------------------------------------------------
Best Loss: 0.00027506213866039536
Best Parameters: [2.6514477024161742, 0.6011515230827974, 0.4529581100522778, 437.94642760340304, 2.132456121480403, 0.5978519925072291, 0.6552364418199805]

Optimizing Learning Rate Schedule:
Iteration 0, loss: 2.9024662888076596
[0.0003     0.00029999 0.00029999 0.00029998 0.00029998]  ...  [0.00019082 0.00019082 0.00019081 0.00019081 0.0001908 ]
Last 5-step gradients: tensor([-113.2932,  -96.9792,  -78.2666,  -56.5247,  -30.8665],
       dtype=torch.float64)
Gradient norm: 39500.393619405746
Iteration 1000, loss: 2.794275536625543
[0.0003 0.0003 0.0003 0.0003 0.0003]  ...  [1.19838650e-06 1.17370362e-06 1.14889767e-06 1.12392698e-06
 1.09875479e-06]
Last 5-step gradients: tensor([ 0.0038,  0.0018,  0.0004, -0.0004, -0.0005], dtype=torch.float64)
Gradient norm: 26191.831871894596
Iteration 2000, loss: 2.7941371243303803
[0.0003 0.0003 0.0003 0.0003 0.0003]  ...  [1.18981265e-06 1.16532158e-06 1.14064852e-06 1.11576900e-06
 1.09070521e-06]
Last 5-step gradients: tensor([0.0155, 0.0117, 0.0084, 0.0056, 0.0029], dtype=torch.float64)
Gradient norm: 26211.70700664234
Iteration 3000, loss: 2.7940660158457735
[0.0003 0.0003 0.0003 0.0003 0.0003]  ...  [1.21978838e-06 1.21978838e-06 1.21978838e-06 1.21978838e-06
 1.21978838e-06]
Last 5-step gradients: tensor([3.5248, 2.5448, 1.6958, 0.9840, 0.4163], dtype=torch.float64)
Gradient norm: 26215.32391310833
Iteration 4000, loss: 2.794070697277415
[0.0003 0.0003 0.0003 0.0003 0.0003]  ...  [1.86508304e-06 1.86508304e-06 1.86508304e-06 1.86508304e-06
 1.86508304e-06]
Last 5-step gradients: tensor([18.3844, 14.4929, 10.7043,  7.0227,  3.4530], dtype=torch.float64)
Gradient norm: 33978.83303919025
Iteration 5000, loss: 2.794059847517886
[0.0003 0.0003 0.0003 0.0003 0.0003]  ...  [1.40065581e-06 1.40065581e-06 1.40065581e-06 1.40065581e-06
 1.40065581e-06]
Last 5-step gradients: tensor([9.5927, 7.4237, 5.3742, 3.4497, 1.6562], dtype=torch.float64)
Gradient norm: 28889.26504205463
Iteration 6000, loss: 2.794122429924052
[0.0003 0.0003 0.0003 0.0003 0.0003]  ...  [1.20838558e-06 1.18208487e-06 1.15727542e-06 1.13374372e-06
 1.11108645e-06]
Last 5-step gradients: tensor([0.0302, 0.0344, 0.0267, 0.0130, 0.0012], dtype=torch.float64)
Gradient norm: 26156.69064751553
Iteration 7000, loss: 2.7941156589592695
[0.0003 0.0003 0.0003 0.0003 0.0003]  ...  [1.20699618e-06 1.19264741e-06 1.16696815e-06 1.13788468e-06
 1.11459469e-06]
Last 5-step gradients: tensor([0.5399, 0.2694, 0.1299, 0.0784, 0.0487], dtype=torch.float64)
Gradient norm: 26171.24734571102
Iteration 8000, loss: 2.794103179365758
[0.0003 0.0003 0.0003 0.0003 0.0003]  ...  [9.70037193e-07 9.70037193e-07 9.70037193e-07 9.70037193e-07
 9.70037193e-07]
Last 5-step gradients: tensor([8.8118, 6.8121, 4.9258, 3.1580, 1.5141], dtype=torch.float64)
Gradient norm: 26173.87222776851
Iteration 9000, loss: 2.7940578037918398
[0.0003 0.0003 0.0003 0.0003 0.0003]  ...  [1.23202323e-06 1.23202323e-06 1.23202323e-06 1.23202323e-06
 1.23202323e-06]
Last 5-step gradients: tensor([2.8457, 1.9999, 1.2858, 0.7098, 0.2787], dtype=torch.float64)
Gradient norm: 26200.50288958315
Final loss: 2.794136776132377
Optimized Learning Rate Schedule:
[0.0003 0.0003 0.0003 0.0003 0.0003]  ...  [1.19868059e-06 1.17369373e-06 1.14832125e-06 1.12265913e-06
 1.09662072e-06]
