
==================================================
Initializing with parameters: (np.float64(3.1477625719295133), np.float64(0.5256129268305814), np.float64(0.49872304692376185), np.float64(378.18319836712703), 1.0, 0.5, 0.5)
==================================================

Step    0: Loss=0.013389, Best Loss=0.013389, Grad Norm=1.50e-01
Parameters: L0=3.0962, A=0.4754, alpha=0.5037, B=377.9441, C=0.9495, beta=0.4950, gamma=0.4950

Step   10: Loss=0.001844, Best Loss=0.001844, Grad Norm=1.15e-01
Parameters: L0=3.0578, A=0.4783, alpha=0.5154, B=376.0419, C=1.0943, beta=0.4993, gamma=0.5087

Step   20: Loss=0.001565, Best Loss=0.000449, Grad Norm=1.39e-01
Parameters: L0=3.0382, A=0.5077, alpha=0.5258, B=374.1859, C=1.2872, beta=0.5088, gamma=0.5289

Step   30: Loss=0.000902, Best Loss=0.000449, Grad Norm=1.16e-01
Parameters: L0=3.0442, A=0.5383, alpha=0.5275, B=372.3803, C=1.4518, beta=0.5196, gamma=0.5478

Step   40: Loss=0.000382, Best Loss=0.000382, Grad Norm=4.53e-02
Parameters: L0=3.0490, A=0.5247, alpha=0.5194, B=370.6064, C=1.5657, beta=0.5290, gamma=0.5626

Step   50: Loss=0.000455, Best Loss=0.000312, Grad Norm=9.90e-02
Parameters: L0=3.0457, A=0.5209, alpha=0.5159, B=368.8629, C=1.7185, beta=0.5427, gamma=0.5845

Step   60: Loss=0.000368, Best Loss=0.000296, Grad Norm=6.36e-02
Parameters: L0=3.0425, A=0.5235, alpha=0.5139, B=367.1509, C=1.8681, beta=0.5578, gamma=0.6073

Step   70: Loss=0.000279, Best Loss=0.000279, Grad Norm=5.12e-03
Parameters: L0=3.0422, A=0.5215, alpha=0.5100, B=365.4613, C=1.9781, beta=0.5709, gamma=0.6256

Step   80: Loss=0.000279, Best Loss=0.000279, Grad Norm=1.93e-03
Parameters: L0=3.0405, A=0.5247, alpha=0.5079, B=363.7875, C=2.0656, beta=0.5828, gamma=0.6414

Step   90: Loss=0.000284, Best Loss=0.000279, Grad Norm=1.82e-02
Parameters: L0=3.0403, A=0.5257, alpha=0.5050, B=362.1308, C=2.1327, beta=0.5931, gamma=0.6546

Early stopping at step 100: No improvement for 20 steps.
Train Set Evaluation:
cosine_24000.csv
huber_loss: 0.0001467923073360711
mse_loss: 3.8164939308602744e-05
rmse_loss: 0.006177777861707456
mae_loss: 0.0044281068634530844
prede: 0.0012760125319758468
worste: 0.00996088596254288
r2_score: 0.9985072383535708
constant_24000.csv
huber_loss: 0.00010137105026084513
mse_loss: 2.9148365392666173e-05
rmse_loss: 0.005398922614065344
mae_loss: 0.003761619506646527
prede: 0.0010536477890784137
worste: 0.009006955656322306
r2_score: 0.9986735129578099
wsdcon_9.csv
huber_loss: 4.305967947778756e-05
mse_loss: 1.3980559364579383e-05
rmse_loss: 0.0037390586201047162
mae_loss: 0.00292648281118277
prede: 0.000831644912161068
worste: 0.0036622253695862846
r2_score: 0.9994707390575541
Average Huber_loss: 9.707434569156791e-05
Average Mse_loss: 2.70979546886161e-05
Average Rmse_loss: 0.005105253031959172
Average Mae_loss: 0.0037054030604274607
Average Prede: 0.001053768411071776
Average Worste: 0.007543355662817157
Average R2_score: 0.9988838301229782
--------------------------------------------------
Test Set Evaluation:
constant_72000.csv
huber_loss: 8.275750472901335e-05
mse_loss: 3.548609880437851e-06
rmse_loss: 0.001883775432592179
mae_loss: 0.0015795125177865813
prede: 0.00046889913094910995
worste: 0.0020228188361805493
r2_score: 0.9997553024421452
cosine_72000.csv
huber_loss: 0.0009533681782414941
mse_loss: 6.418015218988991e-05
rmse_loss: 0.008011251599462465
mae_loss: 0.00744534674956826
prede: 0.002239722609597285
worste: 0.0070627863946917165
r2_score: 0.9966227691203491
wsd_20000_24000.csv
huber_loss: 9.85037748605829e-05
mse_loss: 1.8467309758089853e-05
rmse_loss: 0.00429736078984414
mae_loss: 0.003408385678530348
prede: 0.000998797970943345
worste: 0.0029792601756465724
r2_score: 0.9992168163273663
wsdld_20000_24000.csv
huber_loss: 8.433614676111216e-05
mse_loss: 1.4650174471425507e-05
rmse_loss: 0.0038275546333691314
mae_loss: 0.003136095358816766
prede: 0.0009152438097478043
worste: 0.0031070380791832952
r2_score: 0.9993591421583491
wsdcon_3.csv
huber_loss: 8.567428443047729e-05
mse_loss: 4.596234809370628e-05
rmse_loss: 0.006779553679535717
mae_loss: 0.004487187658480141
prede: 0.0012851551383490619
worste: 0.00641766790558688
r2_score: 0.9982525051517157
wsdcon_18.csv
huber_loss: 3.126630948097767e-05
mse_loss: 9.687284866622575e-06
rmse_loss: 0.0031124403394479027
mae_loss: 0.0025048188839814807
prede: 0.000705571986662838
worste: 0.0029803402569390067
r2_score: 0.9996058311942232
Average Huber_loss: 0.00022265103308394285
Average Mse_loss: 2.6082646543361992e-05
Average Rmse_loss: 0.004651989412375255
Average Mae_loss: 0.003760224474527263
Average Prede: 0.0011022317743749073
Average Worste: 0.00409498527470467
Average R2_score: 0.9988020610656915
--------------------------------------------------
Best Loss: 0.0002786274134901626
Best Parameters: [3.0404540607184405, 0.524686039368193, 0.5078685704877554, 363.7875162170278, 2.0656081241337327, 0.5827901296148124, 0.6414225733502996]

Optimizing Learning Rate Schedule:
Iteration 0, loss: 3.237534235389084
[0.0003     0.00029999 0.00029999 0.00029998 0.00029998]  ...  [0.00019082 0.00019082 0.00019081 0.00019081 0.0001908 ]
Last 5-step gradients: tensor([-83.3324, -70.8540, -56.7382, -40.6032, -21.9313], dtype=torch.float64)
Gradient norm: 31658.97681944618
Iteration 1000, loss: 3.1497773048453124
[0.0003 0.0003 0.0003 0.0003 0.0003]  ...  [1.47433540e-06 1.44808578e-06 1.42174221e-06 1.39529568e-06
 1.36873620e-06]
Last 5-step gradients: tensor([-0.0016, -0.0014, -0.0011, -0.0007, -0.0004], dtype=torch.float64)
Gradient norm: 22996.986279744946
Iteration 2000, loss: 3.1497414002653503
[0.0003 0.0003 0.0003 0.0003 0.0003]  ...  [1.49214279e-06 1.46565560e-06 1.43911720e-06 1.41236152e-06
 1.38533516e-06]
Last 5-step gradients: tensor([-0.0012, -0.0007, -0.0011, -0.0013, -0.0009], dtype=torch.float64)
Gradient norm: 22960.908888525264
Iteration 3000, loss: 3.149711356053585
[0.0003 0.0003 0.0003 0.0003 0.0003]  ...  [1.2316941e-06 1.2316941e-06 1.2316941e-06 1.2316941e-06 1.2316941e-06]
Last 5-step gradients: tensor([5.8005, 4.4932, 3.2567, 2.0937, 1.0071], dtype=torch.float64)
Gradient norm: 22948.143573502977
Iteration 4000, loss: 3.1496320369222692
[0.0003 0.0003 0.0003 0.0003 0.0003]  ...  [1.63312071e-06 1.61159469e-06 1.57434684e-06 1.52027457e-06
 1.45227624e-06]
Last 5-step gradients: tensor([-0.6704, -0.5498, -0.3995, -0.2310, -0.0806], dtype=torch.float64)
Gradient norm: 22547.46932898783
Iteration 5000, loss: 3.14964276480188
[0.0003 0.0003 0.0003 0.0003 0.0003]  ...  [1.29263650e-06 1.29253684e-06 1.25197132e-06 1.21386951e-06
 1.20052055e-06]
Last 5-step gradients: tensor([-6.4783, -5.3214, -4.1726, -2.8345, -1.4394], dtype=torch.float64)
Gradient norm: 17810.835206412663
Iteration 6000, loss: 3.149739784084577
[0.0003 0.0003 0.0003 0.0003 0.0003]  ...  [1.50294380e-06 1.47613748e-06 1.44890515e-06 1.42132954e-06
 1.39349135e-06]
Last 5-step gradients: tensor([-0.0043, -0.0041, -0.0028, -0.0009,  0.0003], dtype=torch.float64)
Gradient norm: 22933.780879198366
Iteration 7000, loss: 3.149735675853012
[0.0003 0.0003 0.0003 0.0003 0.0003]  ...  [1.50429663e-06 1.47824503e-06 1.45187465e-06 1.42538820e-06
 1.39770725e-06]
Last 5-step gradients: tensor([ 0.0043,  0.0023, -0.0017, -0.0022, -0.0022], dtype=torch.float64)
Gradient norm: 22936.871934754858
Iteration 8000, loss: 3.1497272347452863
[0.0003 0.0003 0.0003 0.0003 0.0003]  ...  [1.30399842e-06 1.30399842e-06 1.30399842e-06 1.30399842e-06
 1.30399842e-06]
Last 5-step gradients: tensor([4.7116, 3.6163, 2.5945, 1.6491, 0.7832], dtype=torch.float64)
Gradient norm: 22939.864398003625
Iteration 9000, loss: 3.149720656540205
[0.0003 0.0003 0.0003 0.0003 0.0003]  ...  [1.34162324e-06 1.34162324e-06 1.34162324e-06 1.34162324e-06
 1.34162324e-06]
Last 5-step gradients: tensor([3.2311, 2.4290, 1.7017, 1.0523, 0.4839], dtype=torch.float64)
Gradient norm: 22940.8151482759
Final loss: 3.1496773780559524
Optimized Learning Rate Schedule:
[0.0003 0.0003 0.0003 0.0003 0.0003]  ...  [1.47338251e-06 1.44761627e-06 1.41834267e-06 1.38851507e-06
 1.36079518e-06]
